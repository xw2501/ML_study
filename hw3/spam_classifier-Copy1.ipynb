{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## get word dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import glob\n",
    "\n",
    "N = 25\n",
    "def count_words():\n",
    "    root_dir = os.getcwd()\n",
    "    ham_dict = []    # dictionary of ham mails\n",
    "    ham_cont = []    # word frequency in a mail\n",
    "    ham_appe = []    # word appearance times across mails\n",
    "    spam_dict = []\n",
    "    spam_cont = []\n",
    "    spam_appe = []\n",
    "    os.chdir('./train_data/ham')\n",
    "    data_train = glob.glob('*')\n",
    "    for name in data_train:\n",
    "        with open(name) as text_file:\n",
    "            text = text_file.read()\n",
    "            temp_dict = []\n",
    "            temp_cont = []\n",
    "            for word in text.split():\n",
    "                if word.isdigit():\n",
    "                    word = str(len(word))\n",
    "                elif word.isalpha():\n",
    "                    word = word.lower()\n",
    "                else:\n",
    "                    #word = \"*\"\n",
    "                    continue\n",
    "                if word in temp_dict:\n",
    "                    temp_cont[temp_dict.index(word)] += 1\n",
    "                else:\n",
    "                    temp_dict.append(word)\n",
    "                    temp_cont.append(1)\n",
    "            for word in temp_dict:\n",
    "                temp_cont[temp_dict.index(word)] /= (sum(temp_cont) + 1)\n",
    "                if word in ham_dict:\n",
    "                    ham_cont[ham_dict.index(word)] += temp_cont[temp_dict.index(word)]\n",
    "                    ham_appe[ham_dict.index(word)] += 1\n",
    "                else:\n",
    "                    ham_dict.append(word)\n",
    "                    ham_cont.append(temp_cont[temp_dict.index(word)])\n",
    "                    ham_appe.append(1)\n",
    "    os.chdir(root_dir)\n",
    "    \n",
    "    os.chdir('./train_data/spam')\n",
    "    data_train = glob.glob('*')\n",
    "    for name in data_train:\n",
    "        with open(name) as text_file:\n",
    "            text = text_file.read()\n",
    "            temp_dict = []\n",
    "            temp_cont = []\n",
    "            for word in text.split():\n",
    "                if word.isdigit():\n",
    "                    word = str(len(word))\n",
    "                elif word.isalpha():\n",
    "                    word = word.lower()\n",
    "                else:\n",
    "                    #word = \"*\"\n",
    "                    continue\n",
    "                if word in temp_dict:\n",
    "                    temp_cont[temp_dict.index(word)] += 1\n",
    "                else:\n",
    "                    temp_dict.append(word)\n",
    "                    temp_cont.append(1)\n",
    "            for word in temp_dict:\n",
    "                temp_cont[temp_dict.index(word)] /= (sum(temp_cont) + 1)\n",
    "                if word in spam_dict:\n",
    "                    spam_cont[spam_dict.index(word)] += temp_cont[temp_dict.index(word)]\n",
    "                    spam_appe[spam_dict.index(word)] += 1\n",
    "                else:\n",
    "                    spam_dict.append(word)\n",
    "                    spam_cont.append(temp_cont[temp_dict.index(word)])\n",
    "                    spam_appe.append(1)\n",
    "    os.chdir(root_dir)\n",
    "    \n",
    "    return ham_dict, ham_cont, spam_dict, spam_cont, ham_appe, spam_appe\n",
    "    \n",
    "    \n",
    "def merge_dict(dict1, dict2):\n",
    "    dict_out = dict1\n",
    "    for word in dict2:\n",
    "        if word not in dict_out:\n",
    "            dict_out.append(word)\n",
    "    \n",
    "    return dict_out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "15150\n",
      "33510\n"
     ]
    }
   ],
   "source": [
    "ham_dict, ham_cont, spam_dict, spam_cont, ham_appe, spam_appe = count_words()\n",
    "ham_len = len(ham_dict)\n",
    "spam_len = len(spam_dict)\n",
    "print(ham_len)\n",
    "print(spam_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "words = merge_dict(ham_dict, spam_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "from heapq import nlargest\n",
    "\n",
    "spam_top = nlargest(N, spam_cont)\n",
    "spam_ = []\n",
    "for i in range(N):\n",
    "    spam_.append(spam_dict[spam_cont.index(spam_top[i])])\n",
    "    \n",
    "ham_top = nlargest(N, ham_cont)\n",
    "ham_ = []\n",
    "for i in range(N):\n",
    "    ham_.append(ham_dict[ham_cont.index(ham_top[i])])\n",
    "    \n",
    "words = merge_dict(ham_, spam_)\n",
    "print(words)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "import numpy as np\n",
    "from heapq import nlargest\n",
    "\n",
    "word_dict = []\n",
    "word_diff = []\n",
    "word_diff_abs = []\n",
    "\n",
    "for i in range(len(ham_dict)):\n",
    "    ham_cont[i] *= np.log(ham_len/ham_appe[i])/ham_len\n",
    "    # ham_cont[i] /= float(ham_len)\n",
    "    word_dict.append(ham_dict[i])\n",
    "    if ham_dict[i] in spam_dict:\n",
    "        index = spam_dict.index(ham_dict[i])\n",
    "        spam_cont[index] *= np.log(spam_len/spam_appe[index])/spam_len\n",
    "        # spam_cont[index] /= float(spam_len)\n",
    "        word_diff.append(ham_cont[i]-spam_cont[index])\n",
    "        word_diff_abs.append(np.abs(ham_cont[i]-spam_cont[index]))\n",
    "        del spam_cont[index]\n",
    "        del spam_dict[index]\n",
    "        del spam_appe[index]\n",
    "    else:\n",
    "        word_diff.append(ham_cont[i])\n",
    "        word_diff_abs.append(ham_cont[i])\n",
    "        \n",
    "for i in range(len(spam_dict)):\n",
    "    spam_cont[i] *= np.log(spam_len/spam_appe[i])/spam_len\n",
    "    # spam_cont[i] /= float(spam_len)\n",
    "    word_dict.append(spam_dict[i])\n",
    "    word_diff.append(-spam_cont[i])\n",
    "    word_diff_abs.append(spam_cont[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "scrolled": true
   },
   "source": [
    "\n",
    "top_10_diff = nlargest(N, word_diff_abs)\n",
    "words = []\n",
    "for i in range(N):\n",
    "    words.append(word_dict[word_diff_abs.index(top_10_diff[i])])\n",
    "    print(word_diff[word_dict.index(words[i])], \"   \", words[i])\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## load data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import os\n",
    "import glob\n",
    "\n",
    "def load_train(seed=100):\n",
    "    root_dir = os.getcwd()\n",
    "    train_data = []\n",
    "    train_label = []\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    os.chdir('./train_data/ham')\n",
    "    data_train = glob.glob('*')\n",
    "    for name in data_train:\n",
    "        file = []\n",
    "        with open(name) as text_file:\n",
    "            text = text_file.read()\n",
    "            for word in text.split():\n",
    "                if word.isdigit():\n",
    "                    word = str(len(word))\n",
    "                elif word.isalpha():\n",
    "                    word = word.lower()\n",
    "                else:\n",
    "                    word = \"*\"\n",
    "                file.append(word)\n",
    "        train_data.append(file)\n",
    "        train_label.append(0)\n",
    "    os.chdir(root_dir)\n",
    "    \n",
    "    os.chdir('./train_data/spam')\n",
    "    data_train = glob.glob('*')\n",
    "    for name in data_train:\n",
    "        with open(name) as text_file:\n",
    "            file = []\n",
    "            text = text_file.read()\n",
    "            for word in text.split():\n",
    "                if word.isdigit():\n",
    "                    word = str(len(word))\n",
    "                elif word.isalpha():\n",
    "                    word = word.lower()\n",
    "                else:\n",
    "                    word = \"*\"\n",
    "                file.append(word)\n",
    "        train_data.append(file)\n",
    "        train_label.append(1)\n",
    "    os.chdir(root_dir)\n",
    "    \n",
    "    np.random.seed(100)\n",
    "    np.random.shuffle(train_data)\n",
    "    np.random.seed(100)\n",
    "    np.random.shuffle(train_label)\n",
    "    \n",
    "    train_data = np.asarray(train_data)\n",
    "    train_label = np.asarray(train_label)\n",
    "    \n",
    "    indice = np.random.choice(np.arange(train_data.shape[0]), int(train_data.shape[0]*0.1), replace = False)\n",
    "    \n",
    "    test_data = train_data[indice]\n",
    "    test_label = train_label[indice]\n",
    "    train_data = np.delete(train_data, indice)\n",
    "    train_label = np.delete(train_label, indice)\n",
    "    \n",
    "    return train_data, train_label, test_data, test_label\n",
    "\n",
    "def load_test():\n",
    "    root_dir = os.getcwd()\n",
    "    test_data = []\n",
    "    test_label = []\n",
    "    os.chdir('./test_data')\n",
    "    for i in range(800):\n",
    "        name = 'test_email_'+str(i+1)+'.txt'\n",
    "        file = []\n",
    "        with open(name) as text_file:\n",
    "            text = text_file.read()\n",
    "            for word in text.split():\n",
    "                if word.isdigit():\n",
    "                    word = str(len(word))\n",
    "                elif word.isalpha():\n",
    "                    word = word.lower()\n",
    "                else:\n",
    "                    word = \"*\"\n",
    "                file.append(word)\n",
    "        test_data.append(file)\n",
    "        test_label.append(0)\n",
    "    os.chdir(root_dir)\n",
    "    \n",
    "    return test_data, test_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_data, train_label, test_data, test_label = load_train()\n",
    "final_data, final_label = load_test()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## train model and test"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import time\n",
    "\n",
    "def my_training(X_train, y_train, X_val, y_val, final_data, final_label, seed=100, learning_rate=1e-3, epoch=20, batch_size=500, verbose=False, pre_trained_model=None):\n",
    "    print(\"learning_rate={}\".format(learning_rate))\n",
    "\n",
    "    with tf.name_scope('inputs'):\n",
    "        xs = tf.placeholder(shape=[None, len(words)], dtype=tf.float32)\n",
    "        ys = tf.placeholder(shape=[None, ], dtype=tf.int64)\n",
    "\n",
    "    output, loss = build_model(xs, ys)\n",
    "    \n",
    "    iters = int(len(X_train) / batch_size)\n",
    "    step = train_step(loss, learning_rate)\n",
    "    print('number of batches for training: {}'.format(iters))\n",
    "\n",
    "    eve, pre = evaluate(output, ys)\n",
    "    \n",
    "    iter_total = 0\n",
    "    best_acc = 0\n",
    "    cur_model_name = 'myModel_{}'.format(int(time.time()))\n",
    "    prediction = 0\n",
    "    vali = 0\n",
    "\n",
    "    with tf.Session() as sess:\n",
    "        sess.run(tf.global_variables_initializer())\n",
    "\n",
    "        for epc in range(epoch):\n",
    "            print(\"epoch {} \".format(epc + 1))\n",
    "\n",
    "            for itr in range(iters):\n",
    "                iter_total += 1\n",
    "\n",
    "                training_batch_x = X_train[itr * batch_size: (1 + itr) * batch_size]\n",
    "                training_batch_y = y_train[itr * batch_size: (1 + itr) * batch_size]\n",
    "\n",
    "                _, cur_loss = sess.run([step, loss], feed_dict={xs: training_batch_x, ys: training_batch_y})\n",
    "\n",
    "                if iter_total % 100 == 0:\n",
    "                    # do validation\n",
    "                    valid_eve = sess.run(eve, feed_dict={xs: X_val, ys: y_val})\n",
    "                    valid_acc = 100 - valid_eve * 100 / y_val.shape[0]\n",
    "                    if verbose:\n",
    "                        print('{}/{} loss: {} validation accuracy : {}%'.format(\n",
    "                            batch_size * (itr + 1),\n",
    "                            X_train.shape[0],\n",
    "                            cur_loss,\n",
    "                            valid_acc))\n",
    "                    \n",
    "                    # when achieve the best validation accuracy, we store the model paramters\n",
    "                    print('current accuracy! iteration:{} accuracy: {}%'.format(iter_total, valid_acc))\n",
    "                    if valid_acc > best_acc:\n",
    "                        print('Best validation accuracy! iteration:{} accuracy: {}%'.format(iter_total, valid_acc))\n",
    "                        best_acc = valid_acc\n",
    "                        prediction = sess.run(pre, feed_dict={xs: final_data, ys: final_label})\n",
    "        \n",
    "        \n",
    "    print(\"Traning ends. The best valid accuracy is {}.\".format(best_acc))\n",
    "    \n",
    "    return prediction\n",
    "\n",
    "\n",
    "class fc_layer(object):\n",
    "    def __init__(self, input_x, in_size, out_size, rand_seed=100, activation_function=None, index=0):\n",
    "        with tf.variable_scope('fc_layer_%d' % index):\n",
    "            with tf.name_scope('fc_kernel'):\n",
    "                w_shape = [in_size, out_size]\n",
    "                weight = tf.get_variable(name='fc_kernel_%d' % index, shape=w_shape,\n",
    "                                         initializer=tf.glorot_uniform_initializer(seed=rand_seed))\n",
    "                self.weight = weight\n",
    "\n",
    "            with tf.variable_scope('fc_kernel'):\n",
    "                b_shape = [out_size]\n",
    "                bias = tf.get_variable(name='fc_bias_%d' % index, shape=b_shape,\n",
    "                                       initializer=tf.glorot_uniform_initializer(seed=rand_seed))\n",
    "                self.bias = bias\n",
    "\n",
    "            cell_out = tf.add(tf.matmul(input_x, weight), bias)\n",
    "            if activation_function is not None:\n",
    "                cell_out = activation_function(cell_out)\n",
    "            self.cell_out = cell_out\n",
    "            \n",
    "    def output(self):\n",
    "        return self.cell_out\n",
    "    \n",
    "    \n",
    "def evaluate(output, input_y):\n",
    "    with tf.name_scope('evaluate'):\n",
    "        pred = tf.argmax(output, axis=1)\n",
    "        error_num = tf.count_nonzero(pred - input_y, name='error_num')\n",
    "    return error_num, pred\n",
    "\n",
    "\n",
    "def cross_entropy(output, input_y):\n",
    "    with tf.name_scope('cross_entropy'):\n",
    "        label = tf.one_hot(input_y, 5)\n",
    "        ce = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=output))\n",
    "\n",
    "    return ce\n",
    "\n",
    "\n",
    "def train_step(loss, learning_rate=1e-3):\n",
    "    with tf.name_scope('train_step'):\n",
    "        step = tf.train.AdamOptimizer(learning_rate=learning_rate, beta1=0.9, beta2=0.999, \n",
    "                                       epsilon=1e-08, use_locking=False, \n",
    "                                       name='Adam').minimize(loss)\n",
    "        \n",
    "    return step\n",
    "\n",
    "\n",
    "def build_model(x_tf, y_tf):\n",
    "    \n",
    "    fc_layer1 = fc_layer(input_x=x_tf, in_size=len(words), out_size=150, activation_function=tf.nn.relu, index=1)\n",
    "    drop_layer1 = tf.nn.dropout(fc_layer1.output(), 0.5)\n",
    "    \n",
    "    fc_layer2 = fc_layer(input_x=drop_layer1, in_size=150, out_size=50, activation_function=tf.nn.relu, index=2)\n",
    "    drop_layer2 = tf.nn.dropout(fc_layer2.output(), 0.5)\n",
    "    \n",
    "    fc_layer3 = fc_layer(input_x=drop_layer2, in_size=50, out_size=2, activation_function=None, index=3)\n",
    "\n",
    "    logits = fc_layer3.output()\n",
    "\n",
    "    fc_w = [fc_layer1.weight, fc_layer2.weight, fc_layer3.weight]\n",
    "    \n",
    "    with tf.name_scope(\"loss\"):\n",
    "        l2_loss = tf.reduce_sum([tf.norm(w) for w in fc_w])\n",
    "        label = tf.one_hot(y_tf, 2)\n",
    "        cross_entropy_loss = tf.reduce_mean(tf.nn.softmax_cross_entropy_with_logits(labels=label, logits=logits), name='cross_entropy')\n",
    "        loss = tf.add(cross_entropy_loss, 0.01 * l2_loss, name='loss')\n",
    "    \n",
    "    return logits, loss\n",
    "\n",
    "\n",
    "def get_frequency(in_data, word_dict):\n",
    "    out_frequency = []\n",
    "    for sample in in_data:\n",
    "        temp = np.zeros(len(words))\n",
    "        count = 1\n",
    "        for word in sample:\n",
    "            count += 1\n",
    "            if word in word_dict:\n",
    "                temp[word_dict.index(word)] += 1\n",
    "        for i in range(len(words)):\n",
    "            temp[i] = float(temp[i]) / count\n",
    "        out_frequency.append(temp)\n",
    "        \n",
    "    return out_frequency\n",
    "                "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "X_train = get_frequency(train_data, words)\n",
    "X_val = get_frequency(test_data, words)\n",
    "X_test = get_frequency(final_data, words)\n",
    "\n",
    "y_train = train_label\n",
    "y_val = test_label\n",
    "y_test = final_label"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "learning_rate=0.001\n",
      "number of batches for training: 39\n",
      "epoch 1 \n",
      "epoch 2 \n",
      "epoch 3 \n",
      "current accuracy! iteration:100 accuracy: 68.8787185354691%\n",
      "Best validation accuracy! iteration:100 accuracy: 68.8787185354691%\n",
      "epoch 4 \n",
      "epoch 5 \n",
      "epoch 6 \n",
      "current accuracy! iteration:200 accuracy: 77.57437070938215%\n",
      "Best validation accuracy! iteration:200 accuracy: 77.57437070938215%\n",
      "epoch 7 \n",
      "epoch 8 \n",
      "current accuracy! iteration:300 accuracy: 92.44851258581235%\n",
      "Best validation accuracy! iteration:300 accuracy: 92.44851258581235%\n",
      "epoch 9 \n",
      "epoch 10 \n",
      "epoch 11 \n",
      "current accuracy! iteration:400 accuracy: 92.44851258581235%\n",
      "epoch 12 \n",
      "epoch 13 \n",
      "current accuracy! iteration:500 accuracy: 94.05034324942791%\n",
      "Best validation accuracy! iteration:500 accuracy: 94.05034324942791%\n",
      "epoch 14 \n",
      "epoch 15 \n",
      "epoch 16 \n",
      "current accuracy! iteration:600 accuracy: 94.27917620137299%\n",
      "Best validation accuracy! iteration:600 accuracy: 94.27917620137299%\n",
      "epoch 17 \n",
      "epoch 18 \n",
      "current accuracy! iteration:700 accuracy: 94.50800915331808%\n",
      "Best validation accuracy! iteration:700 accuracy: 94.50800915331808%\n",
      "epoch 19 \n",
      "epoch 20 \n",
      "epoch 21 \n",
      "current accuracy! iteration:800 accuracy: 95.4233409610984%\n",
      "Best validation accuracy! iteration:800 accuracy: 95.4233409610984%\n",
      "epoch 22 \n",
      "epoch 23 \n",
      "epoch 24 \n",
      "current accuracy! iteration:900 accuracy: 94.73684210526315%\n",
      "epoch 25 \n",
      "epoch 26 \n",
      "current accuracy! iteration:1000 accuracy: 97.25400457665904%\n",
      "Best validation accuracy! iteration:1000 accuracy: 97.25400457665904%\n",
      "epoch 27 \n",
      "epoch 28 \n",
      "epoch 29 \n",
      "current accuracy! iteration:1100 accuracy: 96.79633867276888%\n",
      "epoch 30 \n",
      "epoch 31 \n",
      "current accuracy! iteration:1200 accuracy: 96.5675057208238%\n",
      "epoch 32 \n",
      "epoch 33 \n",
      "epoch 34 \n",
      "current accuracy! iteration:1300 accuracy: 96.10983981693364%\n",
      "epoch 35 \n",
      "epoch 36 \n",
      "current accuracy! iteration:1400 accuracy: 95.65217391304348%\n",
      "epoch 37 \n",
      "epoch 38 \n",
      "epoch 39 \n",
      "current accuracy! iteration:1500 accuracy: 94.96567505720824%\n",
      "epoch 40 \n",
      "epoch 41 \n",
      "epoch 42 \n",
      "current accuracy! iteration:1600 accuracy: 95.4233409610984%\n",
      "epoch 43 \n",
      "epoch 44 \n",
      "current accuracy! iteration:1700 accuracy: 96.79633867276888%\n",
      "epoch 45 \n",
      "epoch 46 \n",
      "epoch 47 \n",
      "current accuracy! iteration:1800 accuracy: 95.19450800915332%\n",
      "epoch 48 \n",
      "epoch 49 \n",
      "current accuracy! iteration:1900 accuracy: 96.33867276887872%\n",
      "epoch 50 \n",
      "epoch 51 \n",
      "epoch 52 \n",
      "current accuracy! iteration:2000 accuracy: 95.88100686498856%\n",
      "epoch 53 \n",
      "epoch 54 \n",
      "current accuracy! iteration:2100 accuracy: 97.02517162471396%\n",
      "epoch 55 \n",
      "epoch 56 \n",
      "epoch 57 \n",
      "current accuracy! iteration:2200 accuracy: 95.4233409610984%\n",
      "epoch 58 \n",
      "epoch 59 \n",
      "current accuracy! iteration:2300 accuracy: 96.10983981693364%\n",
      "epoch 60 \n",
      "epoch 61 \n",
      "epoch 62 \n",
      "current accuracy! iteration:2400 accuracy: 95.4233409610984%\n",
      "epoch 63 \n",
      "epoch 64 \n",
      "epoch 65 \n",
      "current accuracy! iteration:2500 accuracy: 96.10983981693364%\n",
      "epoch 66 \n",
      "epoch 67 \n",
      "current accuracy! iteration:2600 accuracy: 96.79633867276888%\n",
      "epoch 68 \n",
      "epoch 69 \n",
      "epoch 70 \n",
      "current accuracy! iteration:2700 accuracy: 95.65217391304348%\n",
      "epoch 71 \n",
      "epoch 72 \n",
      "current accuracy! iteration:2800 accuracy: 97.02517162471396%\n",
      "epoch 73 \n",
      "epoch 74 \n",
      "epoch 75 \n",
      "current accuracy! iteration:2900 accuracy: 96.79633867276888%\n",
      "epoch 76 \n",
      "epoch 77 \n",
      "current accuracy! iteration:3000 accuracy: 95.4233409610984%\n",
      "epoch 78 \n",
      "epoch 79 \n",
      "epoch 80 \n",
      "current accuracy! iteration:3100 accuracy: 95.4233409610984%\n",
      "epoch 81 \n",
      "epoch 82 \n",
      "epoch 83 \n",
      "current accuracy! iteration:3200 accuracy: 96.10983981693364%\n",
      "epoch 84 \n",
      "epoch 85 \n",
      "current accuracy! iteration:3300 accuracy: 97.25400457665904%\n",
      "epoch 86 \n",
      "epoch 87 \n",
      "epoch 88 \n",
      "current accuracy! iteration:3400 accuracy: 95.88100686498856%\n",
      "epoch 89 \n",
      "epoch 90 \n",
      "current accuracy! iteration:3500 accuracy: 96.10983981693364%\n",
      "epoch 91 \n",
      "epoch 92 \n",
      "epoch 93 \n",
      "current accuracy! iteration:3600 accuracy: 96.33867276887872%\n",
      "epoch 94 \n",
      "epoch 95 \n",
      "current accuracy! iteration:3700 accuracy: 97.02517162471396%\n",
      "epoch 96 \n",
      "epoch 97 \n",
      "epoch 98 \n",
      "current accuracy! iteration:3800 accuracy: 96.33867276887872%\n",
      "epoch 99 \n",
      "epoch 100 \n",
      "current accuracy! iteration:3900 accuracy: 96.5675057208238%\n",
      "Traning ends. The best valid accuracy is 97.25400457665904.\n"
     ]
    }
   ],
   "source": [
    "prediction = my_training(X_train, y_train, X_val, y_val, X_test, y_test, seed=100, learning_rate=1e-3, epoch=100, batch_size=100, verbose=False, pre_trained_model=None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## write CSV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import csv\n",
    "import os\n",
    "\n",
    "with open('predicted.csv','w') as csvfile:\n",
    "    fieldnames = ['email_id','labels']\n",
    "    writer = csv.DictWriter(csvfile, fieldnames=fieldnames)\n",
    "    writer.writeheader()    \n",
    "    for index,l in enumerate(prediction):\n",
    "        filename = str(index+1)\n",
    "        label = str(l)\n",
    "        writer.writerow({'email_id': filename, 'labels': label})"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
